"""Contains all the utility methods related to llamaindex"""

import os

from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import AutoMergingRetriever
from llama_index.core.node_parser import get_leaf_nodes
from llama_index.core.node_parser import HierarchicalNodeParser
from llama_index.core import ServiceContext, StorageContext, VectorStoreIndex

import numpy as np
from trulens_eval import Feedback, TruLlama, feedback

import nest_asyncio

nest_asyncio.apply()


def _get_feedbacks():
    """Get predefined feedback open ai functions"""

    openai = feedback.AzureOpenAI(
        deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    )

    qa_relevance = Feedback(
        openai.relevance_with_cot_reasons, name="Answer Relevance"
    ).on_input_output()

    qs_relevance = (
        Feedback(openai.relevance_with_cot_reasons, name="Context Relevance")
        .on_input()
        .on(TruLlama.select_source_nodes().node.text)
        .aggregate(np.mean)
    )

    grounded = feedback.Groundedness(groundedness_provider=openai)

    groundedness = (
        Feedback(grounded.groundedness_measure_with_cot_reasons, name="Groundedness")
        .on(TruLlama.select_source_nodes().node.text)
        .on_output()
        .aggregate(grounded.grounded_statements_aggregator)
    )

    feedbacks = [qa_relevance, qs_relevance, groundedness]

    return feedbacks


def get_trulens_recorder(query_engine, custom_feedbacks, app_id):
    """Get trulens recorder

    Args:
        query_engine (obj): llama index query engine object
        custom_feedbacks (list): array of feedback functions for openai
        app_id (str): application identifier for trulens

    Returns:
        obj: retrusn trullama object
    """
    tru_recorder = TruLlama(query_engine, app_id=app_id, feedbacks=custom_feedbacks)
    return tru_recorder


def get_prebuilt_trulens_recorder(query_engine, app_id):
    """Get prebuild trulens recorder

    Args:
         query_engine (obj): llama index query engine object
         app_id (str): application identifier for trulens

     Returns:
         obj: retrusn trullama object
    """
    tru_recorder = TruLlama(query_engine, app_id=app_id, feedbacks=_get_feedbacks())
    return tru_recorder


def build_automerging_index(
    llm, embed_model, vector_store, documents=None, chunk_sizes=None
):
    """Build auto merging index

    Args:
        llm (obj): LLM object. OpenAI or AzureOpenAI
        embed_model (obj): embedding model obj
        vector_store (obj): Vector database store
        documents (list, optional): documets generated by llama index. Defaults to None.
        chunk_sizes (list, optional): list of chunk sizes. Defaults to None.

    Returns:
        obj: build index object and returns
    """
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)

    if len(documents) > 0:
        chunk_sizes = chunk_sizes or [2048, 512, 128]
        node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)
        nodes = node_parser.get_nodes_from_documents(documents)
        leaf_nodes = get_leaf_nodes(nodes)

        storage_context.docstore.add_documents(nodes)

        automerging_index = VectorStoreIndex(
            leaf_nodes, storage_context=storage_context, service_context=service_context
        )
    else:
        automerging_index = VectorStoreIndex.from_vector_store(
            vector_store, service_context=service_context
        )
    return automerging_index


def get_automerging_query_engine(
    llm, embed_model, vector_store, similarity_top_k=12, text_qa_template=None
):
    """Get auto merging query engine

    Args:
        llm (obj): llm object OpenAI or AzureOpenAI
        embed_model (obj): embedding model
        vector_store (obj): Vector store object
        similarity_top_k (int, optional): top n number of similar search results. Defaults to 12.
        text_qa_template (str, optional): text q&a prompt. Defaults to None.

    Returns:
        obj: return query engine object
    """
    automerging_index = build_automerging_index(llm, embed_model, vector_store)
    service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)
    base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)
    retriever = AutoMergingRetriever(
        base_retriever, automerging_index.storage_context, verbose=True
    )

    auto_merging_engine = RetrieverQueryEngine.from_args(
        retriever, service_context=service_context, text_qa_template=text_qa_template
    )
    return auto_merging_engine
